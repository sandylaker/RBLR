{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "collapsed": true,
        "pycharm": {}
      },
      "outputs": [],
      "source": "import sys\nsys.path.append(\u0027..\u0027)"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "outputs": [],
      "source": "import numpy as np\nfrom rblr import IFB, ClassicalBootstrap, Preprocessor, \\\n    StratifiedBootstrap, ModifiedStraitifiedBootstrap\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.linear_model import LogisticRegression\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport time\n%matplotlib inline",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "## 1. Load and clean dataset\n### 1.1 load dataset\nThe dataset is the breast cancer dataset from \nhttps://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+%28Diagnostic%29 It contains 569 observations and each \nobservation has 32 features. There are two types of labels. \"B\" stands for benign, \"M\" stands for malignant. \n31 out of 32 features consist of numeric values, while the last feature contains a large number of nan values, and the \nfirst feature is the ID number.",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%% md\n"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "outputs": [
        {
          "data": {
            "text/plain": "         id diagnosis  radius_mean  texture_mean  perimeter_mean  area_mean  \\\n0    842302         M        17.99         10.38          122.80     1001.0   \n1    842517         M        20.57         17.77          132.90     1326.0   \n2  84300903         M        19.69         21.25          130.00     1203.0   \n3  84348301         M        11.42         20.38           77.58      386.1   \n4  84358402         M        20.29         14.34          135.10     1297.0   \n\n   smoothness_mean  compactness_mean  concavity_mean  concave points_mean  \\\n0          0.11840           0.27760          0.3001              0.14710   \n1          0.08474           0.07864          0.0869              0.07017   \n2          0.10960           0.15990          0.1974              0.12790   \n3          0.14250           0.28390          0.2414              0.10520   \n4          0.10030           0.13280          0.1980              0.10430   \n\n   ...  texture_worst  perimeter_worst  area_worst  smoothness_worst  \\\n0  ...          17.33           184.60      2019.0            0.1622   \n1  ...          23.41           158.80      1956.0            0.1238   \n2  ...          25.53           152.50      1709.0            0.1444   \n3  ...          26.50            98.87       567.7            0.2098   \n4  ...          16.67           152.20      1575.0            0.1374   \n\n   compactness_worst  concavity_worst  concave points_worst  symmetry_worst  \\\n0             0.6656           0.7119                0.2654          0.4601   \n1             0.1866           0.2416                0.1860          0.2750   \n2             0.4245           0.4504                0.2430          0.3613   \n3             0.8663           0.6869                0.2575          0.6638   \n4             0.2050           0.4000                0.1625          0.2364   \n\n   fractal_dimension_worst  Unnamed: 32  \n0                  0.11890          NaN  \n1                  0.08902          NaN  \n2                  0.08758          NaN  \n3                  0.17300          NaN  \n4                  0.07678          NaN  \n\n[5 rows x 33 columns]",
            "text/html": "\u003cdiv\u003e\n\u003cstyle scoped\u003e\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n\u003c/style\u003e\n\u003ctable border\u003d\"1\" class\u003d\"dataframe\"\u003e\n  \u003cthead\u003e\n    \u003ctr style\u003d\"text-align: right;\"\u003e\n      \u003cth\u003e\u003c/th\u003e\n      \u003cth\u003eid\u003c/th\u003e\n      \u003cth\u003ediagnosis\u003c/th\u003e\n      \u003cth\u003eradius_mean\u003c/th\u003e\n      \u003cth\u003etexture_mean\u003c/th\u003e\n      \u003cth\u003eperimeter_mean\u003c/th\u003e\n      \u003cth\u003earea_mean\u003c/th\u003e\n      \u003cth\u003esmoothness_mean\u003c/th\u003e\n      \u003cth\u003ecompactness_mean\u003c/th\u003e\n      \u003cth\u003econcavity_mean\u003c/th\u003e\n      \u003cth\u003econcave points_mean\u003c/th\u003e\n      \u003cth\u003e...\u003c/th\u003e\n      \u003cth\u003etexture_worst\u003c/th\u003e\n      \u003cth\u003eperimeter_worst\u003c/th\u003e\n      \u003cth\u003earea_worst\u003c/th\u003e\n      \u003cth\u003esmoothness_worst\u003c/th\u003e\n      \u003cth\u003ecompactness_worst\u003c/th\u003e\n      \u003cth\u003econcavity_worst\u003c/th\u003e\n      \u003cth\u003econcave points_worst\u003c/th\u003e\n      \u003cth\u003esymmetry_worst\u003c/th\u003e\n      \u003cth\u003efractal_dimension_worst\u003c/th\u003e\n      \u003cth\u003eUnnamed: 32\u003c/th\u003e\n    \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n    \u003ctr\u003e\n      \u003cth\u003e0\u003c/th\u003e\n      \u003ctd\u003e842302\u003c/td\u003e\n      \u003ctd\u003eM\u003c/td\u003e\n      \u003ctd\u003e17.99\u003c/td\u003e\n      \u003ctd\u003e10.38\u003c/td\u003e\n      \u003ctd\u003e122.80\u003c/td\u003e\n      \u003ctd\u003e1001.0\u003c/td\u003e\n      \u003ctd\u003e0.11840\u003c/td\u003e\n      \u003ctd\u003e0.27760\u003c/td\u003e\n      \u003ctd\u003e0.3001\u003c/td\u003e\n      \u003ctd\u003e0.14710\u003c/td\u003e\n      \u003ctd\u003e...\u003c/td\u003e\n      \u003ctd\u003e17.33\u003c/td\u003e\n      \u003ctd\u003e184.60\u003c/td\u003e\n      \u003ctd\u003e2019.0\u003c/td\u003e\n      \u003ctd\u003e0.1622\u003c/td\u003e\n      \u003ctd\u003e0.6656\u003c/td\u003e\n      \u003ctd\u003e0.7119\u003c/td\u003e\n      \u003ctd\u003e0.2654\u003c/td\u003e\n      \u003ctd\u003e0.4601\u003c/td\u003e\n      \u003ctd\u003e0.11890\u003c/td\u003e\n      \u003ctd\u003eNaN\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003cth\u003e1\u003c/th\u003e\n      \u003ctd\u003e842517\u003c/td\u003e\n      \u003ctd\u003eM\u003c/td\u003e\n      \u003ctd\u003e20.57\u003c/td\u003e\n      \u003ctd\u003e17.77\u003c/td\u003e\n      \u003ctd\u003e132.90\u003c/td\u003e\n      \u003ctd\u003e1326.0\u003c/td\u003e\n      \u003ctd\u003e0.08474\u003c/td\u003e\n      \u003ctd\u003e0.07864\u003c/td\u003e\n      \u003ctd\u003e0.0869\u003c/td\u003e\n      \u003ctd\u003e0.07017\u003c/td\u003e\n      \u003ctd\u003e...\u003c/td\u003e\n      \u003ctd\u003e23.41\u003c/td\u003e\n      \u003ctd\u003e158.80\u003c/td\u003e\n      \u003ctd\u003e1956.0\u003c/td\u003e\n      \u003ctd\u003e0.1238\u003c/td\u003e\n      \u003ctd\u003e0.1866\u003c/td\u003e\n      \u003ctd\u003e0.2416\u003c/td\u003e\n      \u003ctd\u003e0.1860\u003c/td\u003e\n      \u003ctd\u003e0.2750\u003c/td\u003e\n      \u003ctd\u003e0.08902\u003c/td\u003e\n      \u003ctd\u003eNaN\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003cth\u003e2\u003c/th\u003e\n      \u003ctd\u003e84300903\u003c/td\u003e\n      \u003ctd\u003eM\u003c/td\u003e\n      \u003ctd\u003e19.69\u003c/td\u003e\n      \u003ctd\u003e21.25\u003c/td\u003e\n      \u003ctd\u003e130.00\u003c/td\u003e\n      \u003ctd\u003e1203.0\u003c/td\u003e\n      \u003ctd\u003e0.10960\u003c/td\u003e\n      \u003ctd\u003e0.15990\u003c/td\u003e\n      \u003ctd\u003e0.1974\u003c/td\u003e\n      \u003ctd\u003e0.12790\u003c/td\u003e\n      \u003ctd\u003e...\u003c/td\u003e\n      \u003ctd\u003e25.53\u003c/td\u003e\n      \u003ctd\u003e152.50\u003c/td\u003e\n      \u003ctd\u003e1709.0\u003c/td\u003e\n      \u003ctd\u003e0.1444\u003c/td\u003e\n      \u003ctd\u003e0.4245\u003c/td\u003e\n      \u003ctd\u003e0.4504\u003c/td\u003e\n      \u003ctd\u003e0.2430\u003c/td\u003e\n      \u003ctd\u003e0.3613\u003c/td\u003e\n      \u003ctd\u003e0.08758\u003c/td\u003e\n      \u003ctd\u003eNaN\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003cth\u003e3\u003c/th\u003e\n      \u003ctd\u003e84348301\u003c/td\u003e\n      \u003ctd\u003eM\u003c/td\u003e\n      \u003ctd\u003e11.42\u003c/td\u003e\n      \u003ctd\u003e20.38\u003c/td\u003e\n      \u003ctd\u003e77.58\u003c/td\u003e\n      \u003ctd\u003e386.1\u003c/td\u003e\n      \u003ctd\u003e0.14250\u003c/td\u003e\n      \u003ctd\u003e0.28390\u003c/td\u003e\n      \u003ctd\u003e0.2414\u003c/td\u003e\n      \u003ctd\u003e0.10520\u003c/td\u003e\n      \u003ctd\u003e...\u003c/td\u003e\n      \u003ctd\u003e26.50\u003c/td\u003e\n      \u003ctd\u003e98.87\u003c/td\u003e\n      \u003ctd\u003e567.7\u003c/td\u003e\n      \u003ctd\u003e0.2098\u003c/td\u003e\n      \u003ctd\u003e0.8663\u003c/td\u003e\n      \u003ctd\u003e0.6869\u003c/td\u003e\n      \u003ctd\u003e0.2575\u003c/td\u003e\n      \u003ctd\u003e0.6638\u003c/td\u003e\n      \u003ctd\u003e0.17300\u003c/td\u003e\n      \u003ctd\u003eNaN\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003cth\u003e4\u003c/th\u003e\n      \u003ctd\u003e84358402\u003c/td\u003e\n      \u003ctd\u003eM\u003c/td\u003e\n      \u003ctd\u003e20.29\u003c/td\u003e\n      \u003ctd\u003e14.34\u003c/td\u003e\n      \u003ctd\u003e135.10\u003c/td\u003e\n      \u003ctd\u003e1297.0\u003c/td\u003e\n      \u003ctd\u003e0.10030\u003c/td\u003e\n      \u003ctd\u003e0.13280\u003c/td\u003e\n      \u003ctd\u003e0.1980\u003c/td\u003e\n      \u003ctd\u003e0.10430\u003c/td\u003e\n      \u003ctd\u003e...\u003c/td\u003e\n      \u003ctd\u003e16.67\u003c/td\u003e\n      \u003ctd\u003e152.20\u003c/td\u003e\n      \u003ctd\u003e1575.0\u003c/td\u003e\n      \u003ctd\u003e0.1374\u003c/td\u003e\n      \u003ctd\u003e0.2050\u003c/td\u003e\n      \u003ctd\u003e0.4000\u003c/td\u003e\n      \u003ctd\u003e0.1625\u003c/td\u003e\n      \u003ctd\u003e0.2364\u003c/td\u003e\n      \u003ctd\u003e0.07678\u003c/td\u003e\n      \u003ctd\u003eNaN\u003c/td\u003e\n    \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003cp\u003e5 rows × 33 columns\u003c/p\u003e\n\u003c/div\u003e"
          },
          "metadata": {},
          "output_type": "execute_result",
          "execution_count": 5
        }
      ],
      "source": "df \u003d pd.read_csv(\u0027test_dataset/breast_cancer.csv\u0027)\ndf.head()",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "### 1.2 clean data and encode labels into numeric values\nDrop the two redundant columns. Encode \"B\" into 0 and \"M\" into 1.",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%% md\n"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "outputs": [],
      "source": "# remove redundant columns\ndf.drop([\u0027id\u0027, \u0027Unnamed: 32\u0027], axis\u003d1, inplace\u003dTrue)\n# encode diagnosis\ndf[\u0027diagnosis\u0027] \u003d df[\u0027diagnosis\u0027].map({\u0027B\u0027: 0, \u0027M\u0027: 1})",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "### 1.3 prepare data for train and test\n* extract X and y\n* min-max normalize each feature of X according to $$x^{\u0027} \u003d \\frac{x - min(x)}{max(x) - min(x)}$$\nso that values of each feature are located in range from 0 to 1.\n* split the dataset into train and test datasets, where 0.2 proportion of the dataset will be used as test dataset.",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%% md\n"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "outputs": [],
      "source": "# split X and y\nX \u003d df.drop([\u0027diagnosis\u0027], axis\u003d1)\ny \u003d df[\u0027diagnosis\u0027]\n\n# min-max scale\nmin_max_scaler \u003d MinMaxScaler()\nX \u003d min_max_scaler.fit_transform(X)\n\n# split into train and test datasets\nX_train, X_test, y_train, y_test \u003d train_test_split(X, y, test_size\u003d0.2)",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "### 1.4 define a contaminating function to generate outliers\nGiven a fraction  $\\lambda \u003d N_o/N_{train} $\n (default 0.2), representing the fraction of outliers in clean training dataset, and a factor  $f$\n  representing the fraction of outliers that will be labelled with 1, that is, the outliers lying on the the same side \n  as the majority of class 0. The outliers are generated as follows:\n1. calculate number of outliers : $ N_o \u003d N_{train}\\cdot \\lambda $\n , and number of outliers that will be labelled with 1:  $N^{(0)}_o\u003d N_o\\cdot f$\n , and the number of outliers that will be labelled with 0:  $N^{(1)}_o \u003d N_o − N^{(0)}_o$\n .\n \n2. Split the training data into two classes accoring to the label, and randomly sample from each class  $N^{(0)}_o$,\n$N^{(1)}_o$ number of outliers respectively. We obtain two sampled datasets  $X^{(0)}_s$, $X^{(1)}_s$.\n3. for each observation $\\mathbf{x}$ in the sampled datasets, generate an oulier as follows, take an example of class 0:\n$$\\mathbf{x}_o\u003d t_{scale}\\cdot \\mathbf{x} + \\mathbf{x}_{noise}$$ where $t_{scale}$ (default 10) is a magnifer factor \nand $x_{noise}$ is the gaussian noise. Then, reverse the label, i.e. label the outlier with 1.\n4. concatenate the  $\\mathbf{X}$ and label matrix with training data\n",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%% md\n"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "outputs": [],
      "source": "# define a contamination function\ndef contaminate(X_train, y_train, contamination\u003d0.2, scale\u003d10, label_percentage\u003d0.5):\n    if ((np.max(X_train, axis\u003d0) - np.min(X_train, axis\u003d0)) \u003e1.1).any():\n        print(np.max(X_train, axis\u003d0) - np.min(X_train, axis\u003d0))\n        raise ValueError(\"the input matrix is not min-max scaled\")\n    if label_percentage \u003c 0 or label_percentage \u003e1:\n        raise ValueError(\"label percentage can only be between 0 and 1\")\n    np.random.seed()\n    n_out \u003d int(X_train.shape[0] * contamination)\n    n_out_0 \u003d int(n_out * label_percentage)\n    n_out_1 \u003d n_out - n_out_0\n    \n    # generate outliers\n    X_cat_0 \u003d X_train[y_train\u003d\u003d0]\n    X_out_0 \u003d scale * X_cat_0[np.random.choice(X_cat_0.shape[0], n_out_0)] + \\\n              0.3 * scale * np.random.randn(n_out_0, X_cat_0.shape[1])\n    # reverse label of 0-class into 1\n    y_out_0 \u003d np.ones(n_out_0, dtype\u003dint)\n    \n    X_cat_1 \u003d X_train[y_train\u003d\u003d1]\n    X_out_1 \u003d scale * X_cat_1[np.random.choice(X_cat_1.shape[0], n_out_1)] + \\\n              0.3 * scale * np.random.rand(n_out_1, X_cat_1.shape[1])\n    # reverse label of 1-class into 0\n    y_out_1 \u003d np.zeros(n_out_1, dtype\u003dint)\n    \n    # concatenate X and y\n    X_train \u003d np.concatenate((X_train, X_out_0, X_out_1), axis\u003d0)\n    y_train \u003d np.concatenate((y_train, y_out_0, y_out_1), axis\u003d0)\n    return X_train, y_train",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "outputs": [],
      "source": "contamination \u003d 0.3\nX_train_conta, y_train_conta \u003d contaminate(X_train, y_train, contamination, scale\u003d100)",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "## 2. Classification with various models and methods\nUse different models and methods to fit the clean and contaminated data with contamination factor 0.3 respectively, \nthen predict the test dataset and evaluate the accuracies of the models.\n### 2.1 Classical Logisitc regression",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%% md\n"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "outputs": [
        {
          "name": "stdout",
          "text": [
            "classical LR on clean data, accuracy: 0.93\n"
          ],
          "output_type": "stream"
        }
      ],
      "source": "# classical LR on clean data\nclassical_lr \u003d LogisticRegression(solver\u003d\u0027lbfgs\u0027, max_iter\u003d500)\nclassical_lr.fit(X_train, y_train)\nprint(\u0027classical LR on clean data, accuracy: %.2f\u0027 % (classical_lr.score(X_test, y_test)))",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "outputs": [
        {
          "name": "stdout",
          "text": [
            "classical LR on 0.30 contaminated data, accuracy: 0.61\n"
          ],
          "output_type": "stream"
        }
      ],
      "source": "# classical LR on contaminated data\nclassical_lr.fit(X_train_conta, y_train_conta)\nprint(\"classical LR on %.2f contaminated data, accuracy: %.2f\" \n      %(contamination, classical_lr.score(X_test, y_test)))",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "### 2.2 Classical Bootstrap\n# classical bootstrap on clean data",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%% md\n"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "outputs": [
        {
          "name": "stdout",
          "text": [
            "classical Bootstrap on clean data, accuracy: 0.90\n"
          ],
          "output_type": "stream"
        }
      ],
      "source": "classical_boot \u003d ClassicalBootstrap(max_iter\u003d500)\nclassical_boot.fit(X_train, y_train)\nprint(\u0027classical Bootstrap on clean data, accuracy: %.2f\u0027 % (classical_boot.score(X_test, y_test)))\n",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "outputs": [
        {
          "name": "stdout",
          "text": [
            "classical bootstrap on 0.30 contaminated data, accuracy: 0.40\n"
          ],
          "output_type": "stream"
        }
      ],
      "source": "# classical bootstrap on contaminated data\nclassical_boot.fit(X_train_conta, y_train_conta)\nprint(\u0027classical bootstrap on %.2f contaminated data, accuracy: %.2f\u0027 \n      %(contamination, classical_boot.score(X_test, y_test)))",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "### 2.3 Classical Influence Function Bootstrap",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%% md\n"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "outputs": [
        {
          "name": "stdout",
          "text": [
            "classical IFB on clean data, accuracy: 0.95\n"
          ],
          "output_type": "stream"
        }
      ],
      "source": "# classical IFB method on clean data\nifb \u003d IFB(fit_intercept\u003dTrue)\nifb.fit(X_train, y_train, quantile_factor\u003d0.9, max_iter\u003d500)\nprint(\u0027classical IFB on clean data, accuracy: %.2f\u0027 %(ifb.score(X_test, y_test)))",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "outputs": [
        {
          "name": "stdout",
          "text": [
            "classical IFB on 0.30 contaminated data, accuracy: 0.82\n"
          ],
          "output_type": "stream"
        }
      ],
      "source": "# classical IFB method on contaminated data\nifb.fit(X_train_conta, y_train_conta, quantile_factor\u003d0.9, max_iter\u003d500)\nprint(\u0027classical IFB on %.2f contaminated data, accuracy: %.2f\u0027 \n      %(contamination, ifb.score(X_test, y_test)))",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "### 2.4 Preprocessed Influence Function Bootstrap\n# preprocessed IFB on clean data",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%% md\n"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "outputs": [
        {
          "name": "stdout",
          "text": [
            "preprocessed IFB on clean data, accuracy: 0.95\n"
          ],
          "output_type": "stream"
        }
      ],
      "source": "preprocessor \u003d Preprocessor()\nX_train_prep , y_train_prep \u003d preprocessor.fit_transform(X_train, y_train)\nifb.fit(X_train_prep, y_train_prep)\nprint(\"preprocessed IFB on clean data, accuracy: %.2f\" %(ifb.score(X_test, y_test)))",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "outputs": [
        {
          "name": "stdout",
          "text": [
            "preprocessed IFB on 0.30 contaminated data, accuracy: 0.94\n"
          ],
          "output_type": "stream"
        }
      ],
      "source": "# preprocessed IFB on contaminated data\npreprocessor \u003d Preprocessor()\nX_train_conta_prep, y_train_conta_prep \u003d preprocessor.fit_transform(X_train_conta, y_train_conta)\nifb.fit(X_train_conta_prep, y_train_conta_prep)\nprint(\u0027preprocessed IFB on %.2f contaminated data, accuracy: %.2f\u0027 \n      %(contamination, ifb.score(X_test, y_test)))",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "### 2.5 Stratified Bootstrap\n* Number of strata: 5",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%% md\n"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "outputs": [
        {
          "name": "stdout",
          "text": [
            "stratified bootstrap on clean data, accuracy: 0.90\n"
          ],
          "output_type": "stream"
        }
      ],
      "source": "# stratified bootstrap on clean data\nstratified_boot \u003d StratifiedBootstrap()\nstratified_boot.fit(X_train, y_train, n_strata\u003d5, metric\u003d\u0027residual\u0027)\nprint(\u0027stratified bootstrap on clean data, accuracy: %.2f\u0027\n      % (stratified_boot.score(X_test, y_test)))",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "* Number of bootstrap samples: 10\n* Number of strata: 5\n* **Note**: here 10 parallelisms are used. The time effectiveness is somehow unstable, depending on the dataset and bootstrap \nsamples. In good cases the perfomance can reach 147 seconds.",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%% md\n"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "outputs": [
        {
          "name": "stdout",
          "text": [
            "L-BFGS failed to converge: 0 / 10 times\nstratified boostrap on 0.30 contaminated data, accuracy: 0.87\nconsumed time: 172.94 s\n"
          ],
          "output_type": "stream"
        }
      ],
      "source": "# stratified bootstrap on contaminated data\nt1 \u003d time.time()\nstratified_boot.fit(X_train_conta, y_train_conta,\n                    n_bootstrap\u003d10, n_strata\u003d5, metric\u003d\u0027residual\u0027,\n                    verbose\u003dTrue, n_jobs\u003d10)\nprint(\u0027stratified boostrap on %.2f contaminated data, accuracy: %.2f\u0027\n      %(contamination, stratified_boot.score(X_test, y_test)))\nprint(\"consumed time: %.2f s\" % (time.time() - t1))",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "### 2.6 Modified Stratified Bootstrap\n* Number of bootstrap samples: 20\n* Number of strata: 2",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%% md\n"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "outputs": [
        {
          "name": "stdout",
          "text": [
            "modified stratified bootstrap on clean data, accuracy: 0.90\n"
          ],
          "output_type": "stream"
        }
      ],
      "source": "# modified stratified bootstrap on clean data\nmodified_strat_boot \u003d ModifiedStraitifiedBootstrap()\nmodified_strat_boot.fit(X_train, y_train, n_bootstrap\u003d20, n_strata\u003d2)\nprint(\u0027modified stratified bootstrap on clean data, accuracy: %.2f\u0027\n      %(modified_strat_boot.score(X_test, y_test)))",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "On contaminated data:\n* Number of bootstrap samples: 20\n* Number of strata: 3",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%% md\n"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "outputs": [
        {
          "name": "stdout",
          "text": [
            "modified stratified bootstrap on 0.30 contaminated data, accuracy: 0.89\n"
          ],
          "output_type": "stream"
        }
      ],
      "source": "# modified stratified bootstrap on contaminated data\nmodified_strat_boot \u003d ModifiedStraitifiedBootstrap(max_iter\u003d500)\nmodified_strat_boot.fit(X_train_conta, y_train_conta, n_bootstrap\u003d20, n_strata\u003d3)\nprint(\u0027modified stratified bootstrap on %.2f contaminated data, accuracy: %.2f\u0027\n      %(contamination, modified_strat_boot.score(X_test, y_test)))",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "## 3. Summary\nThe simulation results are summarized as follows:\nMethods\t| clean data | 0.3 contamination \n---|:---:| :---: |\nClassical LR| 0.96 | 0.63\nClassical Bootstrap\t| 0.94 | 0.39\nClassical IFB | 0.97 | 0.80\nPreprocessedIFB | 0.97 | 0.97\nStratified Bootstrap | 0.92 | 0.94\nModified Stratified Bootstrap | 0.94 | 0.94\n\n\n",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%% md\n"
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 2",
      "language": "python",
      "name": "python2"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}